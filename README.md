# Learning in Games: IPD Strategy Optimization

A comprehensive comparison of three computational approaches for learning optimal strategies in the Iterated Prisoner's Dilemma (IPD):

1. **ğŸ¯ PPO** - Proximal Policy Optimization (reinforcement learning)
2. **ğŸ§¬ Evolution** - Memory-One strategy optimization with CMA-ES  
3. **ğŸ¤– Transformer** - Decision Transformer approach (supervised learning)

## ğŸ“‹ Overview

This project implements and compares three distinct machine learning paradigms for strategy learning in game theory. Each approach learns to play against a comprehensive set of classic IPD strategies including TitForTat, AlwaysCooperate, AlwaysDefect, Random, Pavlov, Grudger, and GTFT.

### ğŸ”¬ Unified Experimental Setup
- **Rounds per game**: 100
- **Evaluation matches**: 20 per opponent
- **Opponent strategies**: 7 classic strategies
- **Evaluation seed**: 42 (reproducible results)

## ğŸ—ï¸ Project Structure

```
learning-in-games/
â”œâ”€â”€ ğŸ¤– agents/                        # Core learning algorithms
â”‚   â”œâ”€â”€ ppo/
â”‚   â”‚   â”œâ”€â”€ train_ppo.py             # PPO training with enhanced environment
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”œâ”€â”€ evolution/
â”‚   â”‚   â”œâ”€â”€ train_evolution.py       # CMA-ES evolution of memory-one strategies
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â””â”€â”€ transformer/
â”‚       â”œâ”€â”€ train_transformer.py     # Decision Transformer training
â”‚       â””â”€â”€ __init__.py
â”œâ”€â”€ ğŸ“Š notebooks/                     # Interactive demonstrations
â”‚   â”œâ”€â”€ PPO_Demo.py                  # PPO strategy analysis & visualization
â”‚   â”œâ”€â”€ Evolution_Demo.py            # Evolution strategy analysis & visualization
â”‚   â””â”€â”€ Transformer_Demo.py          # Transformer strategy analysis & visualization
â”œâ”€â”€ ğŸ® env/                          # Game environment
â”‚   â”œâ”€â”€ ipd_env.py                   # IPD environment implementation
â”‚   â”œâ”€â”€ strategies.py                # Classic strategy implementations
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ ğŸ“ˆ results/                      # Training results & evaluations
â”‚   â”œâ”€â”€ ppo/                         # PPO results & visualizations
â”‚   â”œâ”€â”€ evolution/                   # Evolution results & visualizations
â”‚   â””â”€â”€ transformer/                 # Transformer results & visualizations
â”œâ”€â”€ ğŸ”¬ comparison_results/            # Cross-approach comparisons
â”‚   â”œâ”€â”€ comprehensive_results.csv    # Complete performance comparison
â”‚   â”œâ”€â”€ scores_comparison.csv        # Score analysis by opponent
â”‚   â”œâ”€â”€ cooperation_comparison.csv   # Cooperation behavior analysis
â”‚   â””â”€â”€ summary_statistics.csv       # Overall performance summary
â”œâ”€â”€ ğŸ’¾ models/                       # Trained models
â”‚   â”œâ”€â”€ ppo_ipd.zip                 # Trained PPO agent
â”‚   â””â”€â”€ evolved_strategy.pkl         # Evolved memory-one strategy
â”œâ”€â”€ ğŸ“‹ analysis/                     # Comprehensive analysis
â”‚   â”œâ”€â”€ FINAL_ANALYSIS.md           # English comparative analysis
â”‚   â””â”€â”€ FINAL_ANALYSIS_UA.md        # Ukrainian comparative analysis
â”œâ”€â”€ âš™ï¸ compare_approaches.py         # Unified comparison framework
â”œâ”€â”€ ğŸ”§ check_parameters.py           # Parameter validation utility
â”œâ”€â”€ ğŸ“¦ requirements.txt              # Python dependencies
â””â”€â”€ ğŸ“– README.md                     # This file
```

## ğŸš€ Quick Start

### 1. Setup Environment
```bash
# Clone repository
git clone <repository-url>
cd learning-in-games

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### 2. Run Interactive Demos
All models are pre-trained and ready for demonstration:

```bash
# PPO Demo - Reinforcement Learning Analysis
python notebooks/PPO_Demo.py

# Evolution Demo - Memory-One Strategy Analysis  
python notebooks/Evolution_Demo.py

# Transformer Demo - Decision Transformer Analysis
python notebooks/Transformer_Demo.py
```

### 3. Convert to Jupyter Notebooks (Optional)
```bash
pip install jupytext

# Convert .py demos to .ipynb format
jupytext --to notebook notebooks/PPO_Demo.py
jupytext --to notebook notebooks/Evolution_Demo.py  
jupytext --to notebook notebooks/Transformer_Demo.py

# Launch Jupyter
jupyter lab
```

## ğŸ¯ Approach Details

### ğŸ¯ PPO (Proximal Policy Optimization)
- **Type**: Reinforcement Learning
- **Architecture**: Enhanced IPD environment with 25-dimensional observation space
- **Training**: Multi-opponent curriculum against all 7 strategies
- **Strategy**: Learns adaptive policy through reward optimization
- **Strengths**: Excellent against reciprocal strategies, strong overall performance

### ğŸ§¬ Evolution (CMA-ES Memory-One)
- **Type**: Evolutionary Optimization
- **Strategy**: Memory-One with 5 parameters (p_cc, p_cd, p_dc, p_dd, initial_action)
- **Training**: CMA-ES optimization over 50 generations with population size 20
- **Behavior**: Mathematically interpretable conditional cooperation probabilities
- **Strengths**: Consistent performance, interpretable strategy parameters

### ğŸ¤– Transformer (Decision Transformer)
- **Type**: Supervised Learning
- **Architecture**: 32-dim hidden size, 15-step context length, 4 layers
- **Training**: Learns from expert trajectories via imitation learning
- **Strategy**: Sequence-to-sequence decision making
- **Strengths**: Stable training, pattern recognition in opponent behavior

## ğŸ“Š Results Summary

Based on comprehensive evaluation across all opponent strategies:

| Approach | Average Score | Rank | Best Against | Strategy Type |
|----------|---------------|------|--------------|---------------|
| **PPO** | 258.8 | ğŸ¥‡ #1 | Reciprocal opponents | Adaptive RL policy |
| **Evolution** | 233.7 | ğŸ¥ˆ #2 | Tit-for-Tat, Always Defect | Memory-One rules |
| **Transformer** | 217.0 | ğŸ¥‰ #3 | Defensive strategies | Sequence learning |

### ğŸ¯ Key Findings
- **PPO** excels with adaptive learning and handles complex opponent behaviors best
- **Evolution** provides interpretable strategy with consistent reciprocal behavior  
- **Transformer** shows stable learning but requires more sophisticated architectures
- All approaches successfully learn cooperative strategies adapted to opponent types

## ğŸ“ˆ Performance Visualizations

Each demo notebook generates comprehensive visualizations:
- **Performance comparison** across all opponents
- **Cooperation rate analysis** showing adaptive behavior
- **Cross-approach comparisons** highlighting strengths/weaknesses
- **Strategic insights** with detailed behavioral analysis

Results are automatically saved to CSV files for further analysis.

## ğŸ”¬ Analysis & Documentation

Comprehensive analysis available in both English and Ukrainian:

### English Analysis:
- [`analysis/FINAL_ANALYSIS.md`](analysis/FINAL_ANALYSIS.md) - Complete comparative study

### Ukrainian Analysis (Ğ£ĞºÑ€Ğ°Ñ—Ğ½ÑÑŒĞºĞ¾Ñ):
- [`analysis/FINAL_ANALYSIS_UA.md`](analysis/FINAL_ANALYSIS_UA.md) - ĞŸĞ¾Ğ²Ğ½Ğ¸Ğ¹ Ğ¿Ğ¾Ñ€Ñ–Ğ²Ğ½ÑĞ»ÑŒĞ½Ğ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ñ–Ğ·

Each analysis includes:
- **Training methodology** and implementation details
- **Performance metrics** and statistical analysis  
- **Strategic behavior** interpretation and insights
- **Technical assessment** with recommendations
- **Future research directions**

## ğŸ› ï¸ Advanced Usage

### Re-train Models
```bash
# Train PPO agent
python agents/ppo/train_ppo.py

# Evolve memory-one strategy
python agents/evolution/train_evolution.py

# Train Decision Transformer
python agents/transformer/train_transformer.py
```

### Run Complete Comparison
```bash
# Execute unified comparison of all approaches
python compare_approaches.py
```

### Parameter Validation
```bash
# Verify unified parameters across all approaches
python check_parameters.py
```

## ğŸ® Game Environment

The IPD environment supports:
- **Configurable rounds** (default: 100)
- **Multiple opponent strategies** (7 classic strategies implemented)
- **Reproducible results** with fixed seeds
- **Detailed logging** of cooperation rates and scores
- **Flexible observation spaces** for different learning approaches

## ğŸ“š Dependencies

Key dependencies (see `requirements.txt` for complete list):
- **stable-baselines3** - PPO implementation
- **cma** - CMA-ES evolutionary optimization  
- **transformers** - Transformer architecture
- **matplotlib, pandas** - Analysis and visualization
- **numpy** - Numerical computations

## ğŸ¤ Contributing

Contributions welcome! Areas for improvement:
- Additional learning algorithms (A3C, SAC, etc.)
- Extended opponent strategy sets
- Multi-objective optimization
- Tournament-style evaluation
- Advanced transformer architectures

## ğŸ“„ License

This project is part of academic research on learning in games and computational game theory.

---

ğŸ¯ **Explore the interactive demos to see each approach in action!**